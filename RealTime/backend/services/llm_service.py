# LLM æœåŠ¡ - å®æ—¶å¯¹è¯åç«¯ä¸“ç”¨
# æä¾›æµå¼ LLM è°ƒç”¨åŠŸèƒ½

import httpx
from typing import Dict, AsyncGenerator, Optional
import json
import sys
import os

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥é¡¹ç›®é…ç½®
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))))
from config import load_json, SETTINGS_FILE


class LLMService:
    """å®æ—¶å¯¹è¯ LLM æœåŠ¡"""
    
    def __init__(self):
        self._config = None
        self._load_config()
    
    def _load_config(self):
        """ä» system_settings.json åŠ è½½ LLM é…ç½®"""
        try:
            settings = load_json(SETTINGS_FILE)
            llm_config = settings.get("phone_call", {}).get("llm", {})
            
            self._config = {
                "api_url": llm_config.get("api_url", ""),
                "api_key": llm_config.get("api_key", ""),
                "model": llm_config.get("model", ""),
                "temperature": llm_config.get("temperature", 0.8),
                "max_tokens": llm_config.get("max_tokens", 2048)
            }
            print(f"[LLMService] âœ… é…ç½®å·²åŠ è½½: model={self._config['model']}")
        except Exception as e:
            print(f"[LLMService] âŒ åŠ è½½é…ç½®å¤±è´¥: {e}")
            self._config = {}
    
    def get_config(self) -> Dict:
        """è·å–å½“å‰ LLM é…ç½®"""
        if not self._config:
            self._load_config()
        return self._config
    
    async def call_stream(
        self,
        messages: list,
        config: Optional[Dict] = None
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼è°ƒç”¨ LLM API
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}]
            config: å¯é€‰çš„é…ç½®è¦†ç›–
            
        Yields:
            æ–‡æœ¬ç‰‡æ®µ (token)
        """
        cfg = config or self._config
        if not cfg:
            self._load_config()
            cfg = self._config
        
        api_url = cfg.get("api_url", "").strip()
        api_key = cfg.get("api_key", "")
        model = cfg.get("model", "")
        
        if not api_url or not api_key or not model:
            raise ValueError("LLM é…ç½®ä¸å®Œæ•´: api_url, api_key, model")
        
        # è‡ªåŠ¨æ·»åŠ  /chat/completions åç¼€
        if '/chat/completions' not in api_url:
            api_url = api_url.rstrip('/') + '/chat/completions'
        
        request_body = {
            "model": model,
            "messages": messages,
            "temperature": cfg.get("temperature", 0.8),
            "max_tokens": cfg.get("max_tokens", 2048),
            "stream": True
        }
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
            "Accept": "text/event-stream"
        }
        
        print(f"[LLMService] ğŸš€ æµå¼è¯·æ±‚: {model}")
        print(f"[LLMService] ğŸ“ æ¶ˆæ¯æ•°: {len(messages)}")
        print(f"[LLMService] â•â•â•â•â•â•â•â•â•â•â•â• LLM è¯·æ±‚è¯¦æƒ… â•â•â•â•â•â•â•â•â•â•â•â•")
        print(f"[LLMService] API URL: {api_url}")
        print(f"[LLMService] Model: {model}")
        print(f"[LLMService] Temperature: {cfg.get('temperature', 0.8)}")
        print(f"[LLMService] Max Tokens: {cfg.get('max_tokens', 2048)}")
        print(f"[LLMService] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Messages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        for i, msg in enumerate(messages):
            role = msg.get('role', 'unknown')
            content = msg.get('content', '')
            # é™åˆ¶æ¯æ¡æ¶ˆæ¯çš„æ˜¾ç¤ºé•¿åº¦
            content_preview = content[:500] + '...' if len(content) > 500 else content
            print(f"[LLMService] [{i}] {role}:")
            for line in content_preview.split('\n'):
                print(f"[LLMService]     {line}")
        print(f"[LLMService] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

        
        async with httpx.AsyncClient(timeout=120.0) as client:
            async with client.stream(
                "POST",
                api_url,
                headers=headers,
                json=request_body
            ) as response:
                if response.status_code != 200:
                    error_text = await response.aread()
                    raise Exception(f"LLM API é”™è¯¯ {response.status_code}: {error_text.decode()[:200]}")
                
                # è§£æ SSE æµ
                async for line in response.aiter_lines():
                    if not line:
                        continue
                    
                    if line.startswith("data: "):
                        data = line[6:]
                        
                        if data == "[DONE]":
                            break
                        
                        try:
                            chunk = json.loads(data)
                            delta = chunk.get("choices", [{}])[0].get("delta", {})
                            content = delta.get("content", "")
                            
                            if content:
                                yield content
                                
                        except json.JSONDecodeError:
                            continue
        
        print(f"[LLMService] âœ… æµå¼å®Œæˆ")


# å•ä¾‹å®ä¾‹
_llm_service = None

def get_llm_service() -> LLMService:
    """è·å– LLM æœåŠ¡å•ä¾‹"""
    global _llm_service
    if _llm_service is None:
        _llm_service = LLMService()
    return _llm_service
