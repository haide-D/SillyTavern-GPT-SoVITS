/**
 * æŒç»­æ€§åˆ†æå¤„ç†å™¨
 * 
 * èŒè´£:
 * - æ¥æ”¶åç«¯çš„continuous_analysis_requestæ¶ˆæ¯
 * - è°ƒç”¨LLMè¿›è¡Œè§’è‰²çŠ¶æ€åˆ†æ
 * - å°†LLMå“åº”å›ä¼ ç»™åç«¯
 */

import { LLMRequestCoordinator } from './llm_request_coordinator.js';
import { LLM_Client } from './llm_client.js';

export class ContinuousAnalysisHandler {
    constructor() {
        console.log('[ContinuousAnalysisHandler] åˆå§‹åŒ–å®Œæˆ');
    }

    /**
     * å¤„ç†æŒç»­æ€§åˆ†æè¯·æ±‚
     * 
     * @param {Object} message - WebSocketæ¶ˆæ¯
     * @param {string} message.type - 'continuous_analysis_request'
     * @param {string} message.chat_branch - å¯¹è¯åˆ†æ”¯ID
     * @param {number} message.floor - æ¥¼å±‚æ•°
     * @param {string} message.context_fingerprint - ä¸Šä¸‹æ–‡æŒ‡çº¹
     * @param {Array} message.speakers - è¯´è¯äºº
     * @param {string} message.prompt - LLM Prompt
     */
    async handle(message) {
        const {
            chat_branch,
            floor,
            context_fingerprint,
            speakers,
            prompt,
            llm_config
        } = message;

        console.log(`[ContinuousAnalysisHandler] ğŸ“Š å¼€å§‹åˆ†ææ¥¼å±‚ ${floor}`);

        // æ£€æŸ¥ LLM é…ç½®æ˜¯å¦å®Œæ•´
        if (!llm_config?.api_url || !llm_config?.api_key || !llm_config?.model) {
            console.error('[ContinuousAnalysisHandler] âŒ åˆ†æ LLM é…ç½®ä¸å®Œæ•´ï¼');
            console.error('è¯·åœ¨ system_settings.json çš„ analysis_llm ä¸­é…ç½® api_url, api_key, model');
            return;
        }

        try {
            // è°ƒç”¨LLMåˆ†æ - ä½¿ç”¨LLM_Clientè€Œä¸æ˜¯LLMRequestCoordinator
            const llmResponse = await LLM_Client.callLLM({
                api_url: llm_config.api_url,
                api_key: llm_config.api_key,
                model: llm_config.model,
                prompt: prompt,
                temperature: llm_config.temperature || 0.8,
                max_tokens: llm_config.max_tokens || 2000
            });

            console.log('[ContinuousAnalysisHandler] âœ… LLMåˆ†æå®Œæˆ');


            // å›ä¼ ç»“æœåˆ°åç«¯
            await this.sendResultToBackend({
                chat_branch,
                floor,
                context_fingerprint,
                speakers,
                llm_response: llmResponse
            });

        } catch (error) {
            console.error('[ContinuousAnalysisHandler] âŒ åˆ†æå¤±è´¥:', error);

            // é€šçŸ¥åç«¯å¤±è´¥
            await this.sendResultToBackend({
                chat_branch,
                floor,
                context_fingerprint,
                speakers,
                llm_response: null,
                error: error.message
            });
        }
    }

    /**
     * å°†LLMåˆ†æç»“æœå‘é€å›åç«¯
     */
    async sendResultToBackend(data) {
        try {
            // ä»å…¨å±€é…ç½®è·å–åç«¯åœ°å€
            const apiUrl = window.TTS_State?.CACHE?.API_URL || 'http://127.0.0.1:3000';
            const backendUrl = `${apiUrl}/api/continuous_analysis/complete`;


            const response = await fetch(backendUrl, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify(data)
            });

            if (!response.ok) {
                throw new Error(`åç«¯å“åº”é”™è¯¯: ${response.status}`);
            }

            const result = await response.json();
            console.log('[ContinuousAnalysisHandler] âœ… ç»“æœå·²å‘é€åˆ°åç«¯:', result);

        } catch (error) {
            console.error('[ContinuousAnalysisHandler] âŒ å‘é€ç»“æœå¤±è´¥:', error);
        }
    }
}


export default ContinuousAnalysisHandler;
