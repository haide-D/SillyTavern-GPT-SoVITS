/**
 * LLM è¯·æ±‚åè°ƒå™¨
 * 
 * èŒè´£:
 * - å¤„ç†æ¥è‡ªåç«¯çš„ LLM è¯·æ±‚
 * - è°ƒç”¨ LLM å®¢æˆ·ç«¯
 * - å°†ç»“æœè¿”å›åç«¯
 * - ç»Ÿä¸€é”™è¯¯å¤„ç†
 */

import { LLM_Client } from './llm_client.js';
import { PhoneCallAPIClient } from './phone_call_api_client.js';

// âœ… ç”¨äºå»é‡çš„è®°å½•IDé›†åˆ
const _processingEavesdropIds = new Set();
const _processedEavesdropIds = new Set();
const MAX_PROCESSED_IDS = 100;  // æœ€å¤šä¿ç•™çš„å·²å¤„ç†IDæ•°é‡

export class LLMRequestCoordinator {
    /**
     * å¤„ç† LLM ç”Ÿæˆè¯·æ±‚
     * 
     * @param {Object} data - LLM è¯·æ±‚æ•°æ®
     * @param {string} data.call_id - æ¥ç”µID
     * @param {string} data.char_name - è§’è‰²å
     * @param {string} data.prompt - æç¤ºè¯
     * @param {Object} data.llm_config - LLM é…ç½®
     * @param {Array} data.speakers - è¯´è¯äººåˆ—è¡¨
     * @param {string} data.chat_branch - èŠå¤©åˆ†æ”¯ID
     */
    static async handleLLMRequest(data) {
        console.log('[LLMRequestCoordinator] ğŸ“¥ æ”¶åˆ° LLM è¯·æ±‚:', data);

        const { call_id, char_name, caller, prompt, llm_config, speakers, chat_branch } = data;

        try {
            // æ˜¾ç¤ºé€šçŸ¥: ä½¿ç”¨ callerï¼ˆå®é™…æ‰“ç”µè¯çš„äººï¼‰ï¼Œå›é€€åˆ° speakers[0] æˆ– char_name
            const displayName = caller || (speakers && speakers[0]) || char_name;
            this.showNotification(`æ­£åœ¨ä¸º ${displayName} ç”Ÿæˆä¸»åŠ¨ç”µè¯...`);

            // è°ƒç”¨ LLM
            console.log('[LLMRequestCoordinator] ğŸ¤– è°ƒç”¨ LLM...');
            const llmResponse = await LLM_Client.callLLM({
                api_url: llm_config.api_url,
                api_key: llm_config.api_key,
                model: llm_config.model,
                temperature: llm_config.temperature,
                max_tokens: llm_config.max_tokens,
                prompt: prompt
            });

            console.log('[LLMRequestCoordinator] âœ… LLM å“åº”æˆåŠŸ, é•¿åº¦:', llmResponse.length);
            console.log('[LLMRequestCoordinator] LLM å“åº”å†…å®¹ (å‰500å­—ç¬¦):', llmResponse.substring(0, 500));

            // å°†ç»“æœå‘é€å›åç«¯
            await PhoneCallAPIClient.completeGeneration({
                call_id: call_id,
                llm_response: llmResponse,
                chat_branch: chat_branch,
                speakers: speakers,
                char_name: char_name
            });

        } catch (error) {
            console.error('[LLMRequestCoordinator] âŒ å¤„ç†å¤±è´¥:', error);

            // ä¸ŠæŠ¥é”™è¯¯
            await PhoneCallAPIClient.logError({
                error_type: 'llm_request_error',
                error_message: error.message,
                error_stack: error.stack,
                call_id: call_id,
                char_name: char_name,
                llm_config: llm_config,
                raw_llm_response: error.rawResponse
            });

            this.showNotification(`ç”Ÿæˆå¤±è´¥: ${error.message}`, 'error');
        }
    }

    /**
     * å¤„ç†åœºæ™¯åˆ†æ LLM è¯·æ±‚
     * 
     * @param {Object} data - åœºæ™¯åˆ†æè¯·æ±‚æ•°æ®
     */
    static async handleSceneAnalysis(data) {
        console.log('[LLMRequestCoordinator] ğŸ” æ”¶åˆ°åœºæ™¯åˆ†æ LLM è¯·æ±‚:', data);

        const { request_id, char_name, prompt, llm_config, speakers, chat_branch,
            trigger_floor, context_fingerprint, context, user_name } = data;

        try {
            console.log('[LLMRequestCoordinator] ğŸ¤– è°ƒç”¨ LLM è¿›è¡Œåœºæ™¯åˆ†æ...');
            const llmResponse = await LLM_Client.callLLM({
                api_url: llm_config.api_url,
                api_key: llm_config.api_key,
                model: llm_config.model,
                temperature: llm_config.temperature,
                max_tokens: llm_config.max_tokens,
                prompt: prompt
            });

            console.log('[LLMRequestCoordinator] âœ… åœºæ™¯åˆ†æ LLM å“åº”æˆåŠŸ, é•¿åº¦:', llmResponse.length);
            console.log('[LLMRequestCoordinator] åœºæ™¯åˆ†æç»“æœ:', llmResponse.substring(0, 300));

            // å°†ç»“æœå‘é€å›åç«¯
            await PhoneCallAPIClient.completeSceneAnalysis({
                request_id: request_id,
                llm_response: llmResponse,
                chat_branch: chat_branch,
                speakers: speakers,
                trigger_floor: trigger_floor,
                context_fingerprint: context_fingerprint,
                context: context,
                char_name: char_name,
                user_name: user_name
            });

        } catch (error) {
            console.error('[LLMRequestCoordinator] âŒ åœºæ™¯åˆ†æå¤„ç†å¤±è´¥:', error);

            // ä¸ŠæŠ¥é”™è¯¯
            await PhoneCallAPIClient.logError({
                error_type: 'scene_analysis_error',
                error_message: error.message,
                error_stack: error.stack,
                request_id: request_id,
                char_name: char_name,
                llm_config: llm_config
            });
        }
    }

    /**
     * å¤„ç†å¯¹è¯è¿½è¸ª LLM è¯·æ±‚
     * 
     * @param {Object} data - å¯¹è¯è¿½è¸ªè¯·æ±‚æ•°æ®
     */
    static async handleEavesdrop(data) {
        console.log('[LLMRequestCoordinator] ğŸ§ æ”¶åˆ°å¯¹è¯è¿½è¸ª LLM è¯·æ±‚:', data);

        const { record_id, char_name, prompt, llm_config, speakers, chat_branch, scene_description, text_lang } = data;

        // âœ… å»é‡æ£€æŸ¥ï¼šé˜²æ­¢åŒä¸€ä¸ª record_id è¢«å¤„ç†å¤šæ¬¡
        if (_processingEavesdropIds.has(record_id) || _processedEavesdropIds.has(record_id)) {
            console.warn(`[LLMRequestCoordinator] âš ï¸ record_id=${record_id} å·²åœ¨å¤„ç†ä¸­æˆ–å·²å¤„ç†ï¼Œè·³è¿‡é‡å¤è¯·æ±‚`);
            return;
        }

        // æ ‡è®°ä¸ºæ­£åœ¨å¤„ç†
        _processingEavesdropIds.add(record_id);

        try {
            // æ˜¾ç¤ºé€šçŸ¥
            this.showNotification(`æ­£åœ¨ç”Ÿæˆ ${speakers.join(' å’Œ ')} çš„ç§ä¸‹å¯¹è¯...`);

            // è°ƒç”¨ LLM
            console.log('[LLMRequestCoordinator] ğŸ¤– è°ƒç”¨ LLM (å¯¹è¯è¿½è¸ª)...');
            const llmResponse = await LLM_Client.callLLM({
                api_url: llm_config.api_url,
                api_key: llm_config.api_key,
                model: llm_config.model,
                temperature: llm_config.temperature,
                max_tokens: llm_config.max_tokens,
                prompt: prompt
            });

            console.log('[LLMRequestCoordinator] âœ… LLM å“åº”æˆåŠŸ (å¯¹è¯è¿½è¸ª), é•¿åº¦:', llmResponse.length);

            // å°†ç»“æœå‘é€å›åç«¯ (åŒ…å« text_lang é…ç½®)
            await PhoneCallAPIClient.completeEavesdrop({
                record_id: record_id,
                llm_response: llmResponse,
                chat_branch: chat_branch,
                speakers: speakers,
                char_name: char_name,
                text_lang: text_lang || 'zh'
            });

        } catch (error) {
            console.error('[LLMRequestCoordinator] âŒ å¯¹è¯è¿½è¸ªå¤„ç†å¤±è´¥:', error);

            // ä¸ŠæŠ¥é”™è¯¯
            await PhoneCallAPIClient.logError({
                error_type: 'eavesdrop_error',
                error_message: error.message,
                error_stack: error.stack,
                record_id: record_id,
                char_name: char_name,
                llm_config: llm_config
            });

            this.showNotification(`å¯¹è¯è¿½è¸ªç”Ÿæˆå¤±è´¥: ${error.message}`, 'error');
        } finally {
            // âœ… æ¸…ç†å¤„ç†ä¸­çŠ¶æ€ï¼Œæ ‡è®°ä¸ºå·²å¤„ç†
            _processingEavesdropIds.delete(record_id);
            _processedEavesdropIds.add(record_id);

            // é™åˆ¶å·²å¤„ç†IDé›†åˆå¤§å°
            if (_processedEavesdropIds.size > MAX_PROCESSED_IDS) {
                const firstId = _processedEavesdropIds.values().next().value;
                _processedEavesdropIds.delete(firstId);
            }
        }
    }

    /**
     * æ˜¾ç¤ºé€šçŸ¥
     * 
     * @param {string} message - æ¶ˆæ¯å†…å®¹
     * @param {string} type - æ¶ˆæ¯ç±»å‹ (info/success/error)
     */
    static showNotification(message, type = 'info') {
        console.log(`[LLMRequestCoordinator] [${type}] ${message}`);

        // å¦‚æœæœ‰ toastr,ä½¿ç”¨å®ƒ
        if (window.toastr) {
            window.toastr[type](message);
        }

        // è§¦å‘è‡ªå®šä¹‰äº‹ä»¶
        if (window.TTS_Events && window.TTS_Events.emit) {
            window.TTS_Events.emit('llm_coordinator_notification', {
                message: message,
                type: type
            });
        }
    }
}

export default LLMRequestCoordinator;
